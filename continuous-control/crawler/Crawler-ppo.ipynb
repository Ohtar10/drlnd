{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations for completing the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program!  In this notebook, you will learn how to control an agent in a more challenging environment, where the goal is to train a creature with four arms to walk forward.  **Note that this exercise is optional!**\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.registry import default_registry\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Crawler.app\"`\n",
    "- **Windows** (x86): `\"path/to/Crawler_Windows_x86/Crawler.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Crawler_Windows_x86_64/Crawler.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Crawler_Linux/Crawler.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Crawler_Linux/Crawler.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Crawler_Linux_NoVis/Crawler.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Crawler_Linux_NoVis/Crawler.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Crawler.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Crawler.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = UnityEnvironment(file_name='./Crawler_Linux/Crawler.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/luis-ferro/test/unity-mlagents/Playground/Builds/Crawler.x86_64')\n",
    "# env = default_registry['CrawlerStaticTarget'].make()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BehaviorSpec(observation_specs=[ObservationSpec(shape=(126,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='PhysicsBodySensor:Body'), ObservationSpec(shape=(32,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size32')], action_spec=ActionSpec(continuous_size=20, discrete_branches=()))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_name = list(env.behavior_specs)[0]\n",
    "behavior_spec = env.behavior_specs[behavior_name]\n",
    "behavior_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 9\n",
      "Size of each action: 20\n",
      "Number of observations: 2\n",
      "Observation space: (126,)\n",
      "There are 9 agents. Each observes a state with length: (126,)\n",
      "The state if the first agent looks like: \n",
      " DecisionStep(obs=[array([-1.5258789e-05, -7.2157383e-03,  1.5258789e-05, -6.8298455e-06,\n",
      "       -3.4842911e-01, -2.0962533e-04, -9.3733513e-01,  6.6253662e-02,\n",
      "       -1.3015985e-01, -8.9758301e-01,  6.8731236e-01,  2.6850380e-02,\n",
      "        2.5180817e-02, -7.2542858e-01,  1.4569092e-01, -1.7535329e-01,\n",
      "       -1.9746704e+00,  7.3637313e-01,  2.5047736e-02,  2.6944000e-02,\n",
      "       -6.7557472e-01, -8.9758301e-01, -1.3001561e-01, -6.6207886e-02,\n",
      "        5.0243717e-01, -4.9525428e-01, -4.6685135e-01, -5.3322589e-01,\n",
      "       -1.9746704e+00, -1.7414832e-01, -1.4564514e-01,  5.3953069e-01,\n",
      "       -4.6035105e-01, -5.0130951e-01, -4.9565330e-01, -6.6192627e-02,\n",
      "       -1.3012457e-01,  8.9759827e-01,  2.5262259e-02, -7.2547233e-01,\n",
      "       -6.8726772e-01, -2.6736440e-02, -1.4559937e-01, -1.7521238e-01,\n",
      "        1.9746857e+00,  2.7098311e-02, -6.7561036e-01, -7.3634052e-01,\n",
      "       -2.4879264e-02,  8.9761353e-01, -1.2980938e-01,  6.6223145e-02,\n",
      "        4.6801832e-01,  5.3204399e-01,  5.0368911e-01, -4.9415109e-01,\n",
      "        1.9747162e+00, -1.7436552e-01,  1.4566040e-01,  5.0146472e-01,\n",
      "        4.9549124e-01,  5.3968042e-01, -4.6018112e-01, -1.9902604e-04,\n",
      "       -4.9914795e-01, -2.2171598e-05,  9.7393955e-04,  1.5174588e+00,\n",
      "       -8.2537258e-04,  1.8041601e-03, -2.1374981e+00, -8.7486114e-04,\n",
      "       -1.3556617e-03,  1.5281862e+00, -5.9591758e-04, -1.4158917e-03,\n",
      "       -2.0983224e+00,  1.9893446e-04, -4.6450633e-04,  1.5170414e+00,\n",
      "        9.0206810e-04,  2.2920968e-03, -2.1317220e+00,  1.2228710e-03,\n",
      "        9.3532697e-04,  1.5226264e+00,  5.2389811e-04,  1.1219528e-03,\n",
      "       -2.0927689e+00, -1.1286172e-04, -6.8298455e-06, -3.4842911e-01,\n",
      "       -2.0962533e-04, -9.3733513e-01, -6.3547891e-01, -2.7778438e-01,\n",
      "       -2.6323441e-01,  6.7060435e-01, -6.9947988e-02, -5.4439181e-05,\n",
      "        2.1854446e-05,  9.9755067e-01, -6.3351649e-01,  2.7853632e-01,\n",
      "        2.6241729e-01,  6.7246687e-01, -7.2017305e-02,  7.7255260e-07,\n",
      "        7.4092727e-06,  9.9740338e-01, -2.6299143e-01,  6.7070496e-01,\n",
      "        6.3538754e-01,  2.7798060e-01, -6.9965474e-02,  3.6368438e-05,\n",
      "       -3.2216385e-06,  9.9754947e-01, -2.6330498e-01, -6.7078555e-01,\n",
      "       -6.3529670e-01,  2.7769676e-01, -6.9986358e-02, -1.2901427e-05,\n",
      "        6.8696995e-06,  9.9754798e-01], dtype=float32), array([ 1.04837523e+01,  4.10265580e-04, -3.19349647e-01,  4.62874596e-05,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.04789333e+01, -1.37856505e-05,\n",
      "       -3.48429084e-01,  6.97433352e-05,  9.37335193e-01, -1.90734863e-06,\n",
      "       -1.52614832e-01,  3.04292259e+01,  2.14878470e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  5.00000000e-01,  0.00000000e+00,  5.00000000e-01,\n",
      "        0.00000000e+00,  5.00000000e-01,  0.00000000e+00,  5.00000000e-01,\n",
      "        0.00000000e+00,  5.00000000e-01,  0.00000000e+00,  5.00000000e-01,\n",
      "        0.00000000e+00,  5.00000000e-01,  0.00000000e+00,  5.00000000e-01],\n",
      "      dtype=float32)], reward=0.0, agent_id=0, action_mask=None, group_id=0, group_reward=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Reset the Environment\n",
    "env.reset()\n",
    "behavior_spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "# Number of agents\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "num_agents = len(decision_steps)\n",
    "print(f\"Number of agents: {num_agents}\")\n",
    "\n",
    "# Size of each action\n",
    "action_size = behavior_spec.action_spec.continuous_size\n",
    "print(f\"Size of each action: {action_size}\")\n",
    "\n",
    "# Examine the state space\n",
    "obs_specs = behavior_spec.observation_specs\n",
    "num_obs = len(obs_specs)\n",
    "state_size = obs_specs[0].shape\n",
    "print(f\"Number of observations: {num_obs}\")\n",
    "print(f\"Observation space: {state_size}\")\n",
    "print(f\"There are {num_agents} agents. Each observes a state with length: {state_size}\")\n",
    "print(\"The state if the first agent looks like: \\n\", decision_steps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: -0.20532982376537331\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "behavior_name = list(env.behavior_specs)[0]\n",
    "behavior_spec = env.behavior_specs[behavior_name]\n",
    "ds, ts = env.get_steps(behavior_name)\n",
    "states = ds.obs[0]\n",
    "scores = np.zeros(num_agents)\n",
    "dones = np.zeros(num_agents)\n",
    "while True:\n",
    "    # Select random action for each agent\n",
    "    action = behavior_spec.action_spec.random_action(num_agents)\n",
    "    # Set the actions\n",
    "    env.set_actions(behavior_name, action)\n",
    "    # Move the simulation one step ahead\n",
    "    env.step()\n",
    "    # Get the s,a,r,ns tuple\n",
    "    ds, ts = env.get_steps(behavior_name)\n",
    "    if len(ts) > 0:\n",
    "        for agent_id in ts:\n",
    "            scores[agent_id] += ts[agent_id].reward\n",
    "            dones[agent_id] = 1\n",
    "        break\n",
    "    next_states = ds.obs[0]\n",
    "    scores += ds.reward\n",
    "    states = next_states\n",
    "\n",
    "print(f\"Total score (averaged over agents) this episode: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from typing import List\n",
    "\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"prob\", \"val\", \"action\", \"reward\", \"done\"])\n",
    "\n",
    "class PPOMemory:\n",
    "\n",
    "    def __init__(self, memory_size: int = int(1e6), batch_size: int = 256) -> None:\n",
    "        self.trajectories = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.current_trajectory = []\n",
    "        self.size = 0\n",
    "\n",
    "    def store(self, state, action, probs, vals, reward, done) -> None:\n",
    "        self.current_trajectory.append(\n",
    "            Experience(state, probs, vals, action, reward, done)\n",
    "        )\n",
    "\n",
    "    def end_trajectory(self):\n",
    "        self.trajectories.append(self.current_trajectory.copy())\n",
    "        self.current_trajectory = []\n",
    "        self.size = len(self.trajectories)\n",
    "\n",
    "    def clear(self):\n",
    "        self.trajectories = []\n",
    "        self.current_trajectory = []\n",
    "        self.size = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def generate_batches(self, max_t: int) -> List[List[Experience]]:\n",
    "        # First, select trajectories at random\n",
    "        indices = np.arange(self.size, dtype=np.int32)\n",
    "        indices = np.random.choice(indices, size=self.batch_size, replace=False)\n",
    "        trajectories = [self.trajectories[i] for i in indices]\n",
    "\n",
    "        # Second, for each trajectory select a random start step with max_t size\n",
    "        return [self.__select_sub_trajectory(t, max_t) for t in trajectories]\n",
    "    \n",
    "    @staticmethod\n",
    "    def __select_sub_trajectory(trajectory, max_t: int):\n",
    "        steps = len(trajectory)\n",
    "        start = np.random.choice(steps - max_t - 1)\n",
    "        return trajectory[start:start + max_t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = PPOMemory(memory_size=10, batch_size=5)\n",
    "\n",
    "for i in range(50):\n",
    "    for _ in range(10):\n",
    "        s, a, p, v, r, d = np.random.randint(0, 100), np.random.randint(0, 100), np.random.randint(0, 100), np.random.randint(0, 100), np.random.randint(0, 100), np.random.randint(0, 100)\n",
    "        memory.store(s, a, p, v, r, d)\n",
    "    memory.end_trajectory()\n",
    "\n",
    "batches = memory.generate_batches(max_t=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Experience(state=42, prob=4, val=95, action=33, reward=41, done=71),\n",
       "  Experience(state=98, prob=39, val=93, action=95, reward=67, done=67),\n",
       "  Experience(state=10, prob=88, val=17, action=32, reward=47, done=63)],\n",
       " [Experience(state=51, prob=65, val=21, action=72, reward=86, done=91),\n",
       "  Experience(state=1, prob=61, val=23, action=48, reward=55, done=91),\n",
       "  Experience(state=73, prob=85, val=13, action=8, reward=59, done=12)],\n",
       " [Experience(state=95, prob=1, val=80, action=14, reward=27, done=12),\n",
       "  Experience(state=31, prob=93, val=92, action=23, reward=14, done=15),\n",
       "  Experience(state=60, prob=2, val=2, action=76, reward=64, done=3)],\n",
       " [Experience(state=17, prob=38, val=88, action=51, reward=44, done=7),\n",
       "  Experience(state=4, prob=46, val=14, action=15, reward=97, done=20),\n",
       "  Experience(state=47, prob=52, val=4, action=75, reward=28, done=98)],\n",
       " [Experience(state=88, prob=16, val=89, action=60, reward=55, done=88),\n",
       "  Experience(state=1, prob=78, val=49, action=68, reward=89, done=46),\n",
       "  Experience(state=48, prob=66, val=80, action=44, reward=62, done=46)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        learning_rate: float, \n",
    "        fc_units = [512, 512, 512],\n",
    "        device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(*input_dims, fc_units[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_units[0], fc_units[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_units[1], fc_units[2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mu = nn.Linear(fc_units[2], n_actions)\n",
    "        self.sigma = nn.Linear(fc_units[2], 1)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.network(state)\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        return mu, sigma\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        learning_rate: float,\n",
    "        fc_units = [512, 512, 512],\n",
    "        device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(*input_dims, fc_units[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_units[0], fc_units[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_units[1], fc_units[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_units[2], 1)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.network(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        input_dims,\n",
    "        gamma: float=0.99,\n",
    "        learning_rate: float=1e-3,\n",
    "        gae_lambda: float=0.95,\n",
    "        policy_clip: float=0.2,\n",
    "        batch_size: int=256,\n",
    "        memory_size: int=int(1e6),\n",
    "        n_epochs: int = 10,\n",
    "        max_t: int = 5,\n",
    "        device: str = 'cpu'\n",
    "        ) -> None:\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.device = device\n",
    "        self.max_t = max_t\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.actor = Actor(n_actions, input_dims, learning_rate, device=device)\n",
    "        self.critic = Critic(input_dims, learning_rate, device=device)\n",
    "        self.memory = PPOMemory(memory_size, batch_size)\n",
    "\n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def end_trajectory(self):\n",
    "        self.memory.end_trajectory()\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float).to(self.device)\n",
    "\n",
    "        mu, sigma = self.actor(state)\n",
    "        sigma = torch.exp(sigma)\n",
    "        action_probs = torch.distributions.Normal(mu, sigma)\n",
    "        probs = action_probs.sample(sample_shape=torch.Size([1])).squeeze()\n",
    "        # probs = action_probs.sample(sample_shape=torch.Size([self.n_actions]))\n",
    "        \n",
    "        log_probs = action_probs.log_prob(probs).to(self.device)\n",
    "        action = torch.tanh(probs).cpu().data.numpy()\n",
    "        value = self.critic(state)\n",
    "        return action, log_probs, value\n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            # batch of trajectory segments\n",
    "            batch = self.memory.generate_batches(self.max_t)\n",
    "\n",
    "            # advantage per each trajectory segment & step in trajectory segment\n",
    "            # shape: (batch_size, max_t)\n",
    "            advantage = self.__calc_advantage(batch)\n",
    "            for i, t_segment in enumerate(batch):\n",
    "                # \"state\", \"prob\", \"val\", \"action\", \"reward\", \"done\"\n",
    "                states = [t.state for t in t_segment]\n",
    "                old_probs = [t.prob for t in t_segment]\n",
    "                values = [t.val for t in t_segment]\n",
    "                actions = [t.action for t in t_segment]\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                weighted_probs = advantage[i] * prob_ratio\n",
    "                weighted_clipped_probs = torch.clamp(prob_ratio, 1 - self.policy_clip, 1 + self.policy_clip) * advantage[i]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage[i] + values\n",
    "                critic_loss = (returns-critic_value) ** 2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss + 0.5 * critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "            \n",
    "    def __calc_advantage(self, batch):\n",
    "        advantage = []\n",
    "        for i, t_segment in enumerate(batch):\n",
    "            discount = 1.0\n",
    "            a_t = 0\n",
    "            advantage.append([])\n",
    "            for j in range(len(t_segment) - 1):\n",
    "                print(\"Shapes:\")\n",
    "                print(f\"reward: {t_segment[j].reward.shape}\")\n",
    "                a_t += discount * (t_segment[j].reward + self.gamma * t_segment[j+1].val * (1 - int(t_segment[j].done)) - t_segment[j].val)\n",
    "                discount *= self.gamma * self.gae_lambda\n",
    "                advantage[i].append(a_t)\n",
    "        return advantage\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from mlagents_envs.environment import ActionTuple\n",
    "\n",
    "def ppo(\n",
    "    agent: Agent,\n",
    "    env,\n",
    "    n_episodes=1000,\n",
    "    horizon=300,\n",
    "    learn_every=20,\n",
    "):\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    avg_scores = []\n",
    "    solved = False\n",
    "    n_steps = 0\n",
    "    with tqdm(total=n_episodes) as progress:\n",
    "        for i_episode in range(1, n_episodes + 1):\n",
    "            env.reset()\n",
    "            ds, _ = env.get_steps(behavior_name)\n",
    "            states = ds.obs[0]\n",
    "            num_agents = len(ds)\n",
    "            score = np.zeros((num_agents, 1))            \n",
    "            for t in range(horizon):\n",
    "                actions, probs, value = agent.act(states)\n",
    "                action_tuple = ActionTuple(continuous=actions)\n",
    "\n",
    "                env.set_actions(behavior_name, action_tuple)\n",
    "                env.step()\n",
    "                ds, ts = env.get_steps(behavior_name)\n",
    "                dones = np.zeros((num_agents, 1))\n",
    "\n",
    "                if len(ds) > 0:\n",
    "                    next_states = ds.obs[0]\n",
    "                    rewards = ds.reward\n",
    "                    rewards = np.expand_dims(np.asanyarray(rewards), axis=1)\n",
    "\n",
    "                    agent.remember(states, actions, probs, value, rewards, dones)\n",
    "                    states = next_states\n",
    "                    score += rewards\n",
    "                \n",
    "                if len(ts) > 0:\n",
    "                    agent_ids = [ai for ai in ts]\n",
    "                    states = states[agent_ids, :]\n",
    "                    next_states = ts.obs[0]\n",
    "                    rewards = ts.reward\n",
    "                    rewards = np.expand_dims(np.asanyarray(rewards), axis=1)\n",
    "                    dones = np.ones((len(ts), 1))\n",
    "                    agent.remember(states, actions, probs, value, rewards, dones)\n",
    "                    agent.end_trajectory()\n",
    "                    for ai in agent_ids:\n",
    "                        score[ai] += ts[ai].reward\n",
    "                    break\n",
    "                \n",
    "                if len(agent.memory.trajectories) > agent.batch_size  and n_steps % learn_every == 0:\n",
    "                    agent.learn()\n",
    "                    \n",
    "            score = np.mean(score)\n",
    "            scores_window.append(score)\n",
    "            scores.append(score)\n",
    "            avg_score = np.mean(scores_window)\n",
    "            avg_scores.append(avg_score)\n",
    "\n",
    "            progress.set_postfix({\"Avg. Score\": f\"{avg_score: .2f}\"})\n",
    "            progress.update()\n",
    "\n",
    "            if avg_score >= 3000.0:\n",
    "                print(f\"Environment solved at {i_episode} episodes with Avg. Score: {avg_score:.2f}\")\n",
    "                solved = True\n",
    "                break\n",
    "\n",
    "    return scores, avg_scores, solved            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:07<00:37,  2.19it/s, Avg. Score=-0.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "reward: (9, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-e40cc5152925>\u001b[0m in \u001b[0;36mppo\u001b[0;34m(agent, env, n_episodes, horizon, learn_every)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m  \u001b[0;32mand\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlearn_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-0c8786b0d150>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# advantage per each trajectory segment & step in trajectory segment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# shape: (batch_size, max_t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0madvantage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__calc_advantage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_segment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;31m# \"state\", \"prob\", \"val\", \"action\", \"reward\", \"done\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-0c8786b0d150>\u001b[0m in \u001b[0;36m__calc_advantage\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"reward: {t_segment[j].reward.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0ma_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_segment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt_segment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_segment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_segment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0mdiscount\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgae_lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0madvantage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_episodes = 100\n",
    "agent = Agent(\n",
    "    action_size, \n",
    "    state_size,\n",
    "    batch_size=16\n",
    ")\n",
    "scores, avg_scores, solved = ppo(agent, env, n_episodes=n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent.memory.trajectories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = agent.memory.generate_batches(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "reward: (9, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-af5570924de5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"reward: {t_segment[j].reward.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0ma_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_segment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt_segment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_segment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_segment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdiscount\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgae_lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0madvantage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "advantage = []\n",
    "for i, t_segment in enumerate(batch):\n",
    "    discount = 1.0\n",
    "    a_t = 0\n",
    "    advantage.append([])\n",
    "    for j in range(len(t_segment) - 1):\n",
    "        print(\"Shapes:\")\n",
    "        print(f\"reward: {t_segment[j].reward.shape}\")\n",
    "        a_t += discount * (t_segment[j].reward + gamma * t_segment[j+1].val * (1 - int(t_segment[j].done)) - t_segment[j].val)\n",
    "        discount *= gamma * gae_lambda\n",
    "        advantage[i].append(a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42666fd1b51428b2663dd79f5b665a8a5032ed98112cbcf125b1b69c1a75acba"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('drlnd-cont-control-2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
